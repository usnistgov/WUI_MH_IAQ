## Migration Guide: Using Utility Modules in Existing Scripts

This guide demonstrates how to migrate existing analysis scripts to use the new utility modules. We'll use `general_particle_count_comparison.py` as an example.

---

## Overview of Changes

**Before:** Scripts contained 100-300 lines of duplicated utility functions
**After:** Import utilities from centralized modules, focus on analysis logic

**Estimated reduction:** 50-70% fewer lines of boilerplate code per script

---

## Step 1: Update Imports

### BEFORE (lines 75-94 of general_particle_count_comparison.py)
```python
import os
import sys
import datetime
import inspect
import traceback
from pathlib import Path
import pandas as pd
import numpy as np
from bokeh.plotting import figure, show
from bokeh.io import output_notebook, output_file
from bokeh.models import Div, Range1d
from bokeh.layouts import column
from bokeh.palettes import Blues, Greens, Reds

# Add repository root to path for portable data access
script_dir = Path(__file__).parent
repo_root = script_dir.parent
sys.path.insert(0, str(repo_root))

from src.data_paths import get_data_root, get_instrument_path, get_common_file
```

### AFTER
```python
import sys
from pathlib import Path
import pandas as pd
import numpy as np
from bokeh.plotting import show
from bokeh.io import output_notebook, output_file
from bokeh.layouts import column

# Add repository root to path
script_dir = Path(__file__).parent
repo_root = script_dir.parent
sys.path.insert(0, str(repo_root))

from src.data_paths import get_data_root, get_instrument_path, get_common_file

# Import utility functions
from scripts.datetime_utils import (
    apply_time_shift,
    calculate_time_since_garage_closed,
)
from scripts.data_filters import (
    filter_by_status_columns,
    calculate_rolling_average_burn3,
    split_data_by_nan,
)
from scripts.plotting_utils import (
    create_standard_figure,
    get_script_metadata,
    add_event_markers,
    configure_legend,
)
from scripts.data_loaders import (
    load_burn_log,
    get_garage_closed_times,
    get_cr_box_times,
    process_aerotrak_data,
    process_quantaq_data,
    process_smps_data,
)
from scripts.instrument_config import (
    AEROTRAK_CHANNELS,
    sum_quantaq_bins,
    sum_smps_ranges,
)
```

**Changes:**
- ✅ Removed imports only needed for utility functions (datetime, inspect, traceback, etc.)
- ✅ Added imports from utility modules
- ✅ Organized imports by category (standard library, third-party, local, utilities)

---

## Step 2: Remove Utility Function Definitions

### BEFORE (lines 115-458 of general_particle_count_comparison.py)

**DELETE these ~343 lines of function definitions:**

```python
def get_script_metadata():
    """Return a string with script name and execution timestamp"""
    try:
        current_frame = inspect.currentframe()
        # ... 14 lines
    return f"Generated by: {script_name} | Date: {timestamp}"


def apply_time_shift(df, instrument, datetime_column):
    """Apply time shift to datetime column..."""
    # ... 18 lines
    return df


def fix_smps_datetime(smps_data):
    """Fix the datetime creation for SMPS data."""
    # ... 68 lines
    return data


def calculate_time_since_garage_closed(df, datetime_column, garage_closed_time):
    """Calculate the time since garage closed..."""
    # ... 38 lines
    return df


def calculate_rolling_average_burn3(data, datetime_column):
    """Calculate 5-minute rolling average for burn3 data."""
    # ... 51 lines
    return rolling_avg_df


def split_data_by_nan(df, x_col, y_col):
    """Split data into segments where NaN values occur..."""
    # ... 32 lines
    return segments


def create_figure(title):
    """Create a standardized figure for particle count plots."""
    # ... 27 lines
    return p
```

### AFTER

**Delete all of the above** - these functions are now imported from utility modules!

**Line count reduction:** ~343 lines → 0 lines

---

## Step 3: Simplify Burn Log Loading

### BEFORE (lines 462-494)
```python
print("Loading burn log...")
burn_log_path = get_common_file('burn_log')
burn_log = pd.read_excel(burn_log_path, sheet_name="Sheet2")

# Create dictionary to store garage closed times for each burn
garage_closed_times = {}
cr_box_hours = {}

for burn_number in burn_numbers:
    burn_date_row = burn_log[burn_log["Burn ID"] == burn_number]
    if not burn_date_row.empty:
        burn_date = pd.to_datetime(burn_date_row["Date"].iloc[0])

        # Get garage closed time
        garage_closed_time_str = burn_date_row["garage closed"].iloc[0]
        if pd.notna(garage_closed_time_str):
            garage_closed_time = pd.to_datetime(
                f"{burn_date.strftime('%Y-%m-%d')} {garage_closed_time_str}"
            )
            garage_closed_times[burn_number] = garage_closed_time

        # Get CR Box activation time
        cr_box_on_time_str = burn_date_row["CR Box on"].iloc[0]
        if pd.notna(cr_box_on_time_str):
            cr_box_on_time = pd.to_datetime(
                f"{burn_date.strftime('%Y-%m-%d')} {cr_box_on_time_str}"
            )
            # Calculate hours since garage closed for CR box activation
            if burn_number in garage_closed_times:
                cr_box_hours[burn_number] = (
                    cr_box_on_time - garage_closed_times[burn_number]
                ).total_seconds() / 3600
```

### AFTER
```python
print("Loading burn log...")
burn_log_path = get_common_file('burn_log')
burn_log = load_burn_log(burn_log_path)

# Get garage closed times and CR Box activation times
garage_closed_times = get_garage_closed_times(burn_log, burn_numbers)
cr_box_hours = get_cr_box_times(burn_log, burn_numbers, relative_to_garage=True)
```

**Changes:**
- ✅ 33 lines → 5 lines
- ✅ Clearer intent with function names
- ✅ Error handling built into utility functions

---

## Step 4: Simplify Instrument Data Processing

### BEFORE (lines 495-708 - AeroTrak processing)

**~213 lines of repetitive processing code:**

```python
# %% Process AeroTrak data
aerotrak_data = {"Bedroom": {}, "Kitchen": {}}

if "AeroTrak" in selected_instruments:
    print("\nProcessing AeroTrak data...")

    # Load bedroom AeroTrak data
    aerotrak_b_path = get_instrument_path('aerotrak_bedroom') / 'all_data.xlsx'
    aerotrak_k_path = get_instrument_path('aerotrak_kitchen') / 'all_data.xlsx'

    # Process Bedroom AeroTrak
    try:
        print("Loading AeroTrakB data...")
        aerotrak_b_data = pd.read_excel(aerotrak_b_path)
        aerotrak_b_data.columns = aerotrak_b_data.columns.str.strip()

        # Define size channels and initialize a dictionary for size values
        size_channels = ["Ch1", "Ch2", "Ch3", "Ch4", "Ch5", "Ch6"]
        size_values = {}

        # Extract size values for each channel (from first row)
        for channel in size_channels:
            size_col = f"{channel} Size (µm)"
            if size_col in aerotrak_b_data.columns:
                size_value = aerotrak_b_data[size_col].iloc[0]
                if pd.notna(size_value):
                    size_values[channel] = size_value

        # Process each burn
        for burn_number in burn_numbers:
            burn_date_row = burn_log[burn_log["Burn ID"] == burn_number]
            if burn_date_row.empty:
                continue

            burn_date = pd.to_datetime(burn_date_row["Date"].iloc[0])

            # Filter data for this burn
            aerotrak_b_data["Date"] = pd.to_datetime(
                aerotrak_b_data["Date and Time"]
            ).dt.date
            filtered_data = aerotrak_b_data[
                aerotrak_b_data["Date"] == burn_date.date()
            ].copy()

            if filtered_data.empty:
                print(f"No AeroTrakB data for {burn_number}")
                continue

            # Apply time shift
            filtered_data = apply_time_shift(
                filtered_data, "AeroTrakB", "Date and Time"
            )

            # Calculate time since garage closed
            if burn_number in garage_closed_times:
                filtered_data = calculate_time_since_garage_closed(
                    filtered_data, "Date and Time", garage_closed_times[burn_number]
                )

                # Apply rolling average for burn3
                if burn_number == "burn3":
                    filtered_data = calculate_rolling_average_burn3(
                        filtered_data, "Date and Time"
                    )

                # Check instrument status columns and remove rows with invalid status
                status_columns = ["Flow Status", "Laser Status"]
                valid_status = (filtered_data[status_columns] == "OK").all(axis=1)
                filtered_data.loc[
                    ~valid_status,
                    filtered_data.select_dtypes(include=[np.number]).columns,
                ] = np.nan

                # Convert Diff counts to #/cm³
                volume_column = "Volume (L)"
                if volume_column in filtered_data.columns:
                    filtered_data["Volume (cm³)"] = (
                        filtered_data[volume_column] * 1000
                    )  # Convert to cm³

                    # Process each channel
                    for i, channel in enumerate(size_channels):
                        if channel in size_values:
                            # Get next channel's size value
                            next_channel = (
                                size_channels[i + 1]
                                if i < len(size_channels) - 1
                                else None
                            )
                            next_size_value = size_values.get(
                                next_channel, 25
                            )  # Default to 25 if no next channel

                            diff_col = f"{channel} Diff (#)"
                            if diff_col in filtered_data.columns:
                                # Create new column for particle count density
                                new_col_name = f"Ʃ{size_values[channel]}-{next_size_value}µm (#/cm³)"
                                filtered_data[new_col_name] = (
                                    filtered_data[diff_col]
                                    / filtered_data["Volume (cm³)"]
                                )

                # Store processed data
                aerotrak_data["Bedroom"][burn_number] = filtered_data
                print(
                    f"  Processed AeroTrakB data for {burn_number}: {len(filtered_data)} records"
                )

    except (FileNotFoundError, pd.errors.ParserError, KeyError) as e:
        print(f"Error processing AeroTrakB data: {str(e)}")
        traceback.print_exc()

    # [... similar code repeated for Kitchen AeroTrak ...]
```

### AFTER
```python
# %% Process AeroTrak data
aerotrak_data = {"Bedroom": {}, "Kitchen": {}}

if "AeroTrak" in selected_instruments:
    print("\nProcessing AeroTrak data...")

    aerotrak_b_path = get_instrument_path('aerotrak_bedroom') / 'all_data.xlsx'
    aerotrak_k_path = get_instrument_path('aerotrak_kitchen') / 'all_data.xlsx'

    # Process Bedroom AeroTrak
    print("Loading AeroTrakB data...")
    for burn_number in burn_numbers:
        burn_date_row = burn_log[burn_log["Burn ID"] == burn_number]
        if not burn_date_row.empty:
            burn_date = pd.to_datetime(burn_date_row["Date"].iloc[0])
            garage_time = garage_closed_times.get(burn_number)

            filtered_data = process_aerotrak_data(
                aerotrak_b_path,
                instrument="AeroTrakB",
                burn_date=burn_date,
                garage_closed_time=garage_time,
                burn_number=burn_number,
            )

            if not filtered_data.empty:
                aerotrak_data["Bedroom"][burn_number] = filtered_data
                print(f"  Processed AeroTrakB: {burn_number} ({len(filtered_data)} records)")

    # Process Kitchen AeroTrak
    print("Loading AeroTrakK data...")
    for burn_number in burn_numbers:
        burn_date_row = burn_log[burn_log["Burn ID"] == burn_number]
        if not burn_date_row.empty:
            burn_date = pd.to_datetime(burn_date_row["Date"].iloc[0])
            garage_time = garage_closed_times.get(burn_number)

            filtered_data = process_aerotrak_data(
                aerotrak_k_path,
                instrument="AeroTrakK",
                burn_date=burn_date,
                garage_closed_time=garage_time,
                burn_number=burn_number,
            )

            if not filtered_data.empty:
                aerotrak_data["Kitchen"][burn_number] = filtered_data
                print(f"  Processed AeroTrakK: {burn_number} ({len(filtered_data)} records)")
```

**Changes:**
- ✅ ~213 lines → ~35 lines
- ✅ All complexity hidden in `process_aerotrak_data()`
- ✅ Easier to read and understand
- ✅ Consistent error handling
- ✅ Eliminated bedroom/kitchen code duplication

---

## Step 5: Simplify QuantAQ Processing

### BEFORE (lines 709-899 - ~190 lines)
```python
quantaq_data = {"Bedroom": {}, "Kitchen": {}}

if "QuantAQ" in selected_instruments:
    print("\nProcessing QuantAQ data...")

    # Determine which burns have QuantAQ data
    quantaq_burns = [
        burn for burn in burn_numbers if burn not in ["burn1", "burn2", "burn3"]
    ]

    if quantaq_burns:
        # Load QuantAQ data for bedroom and kitchen
        quantaq_b_path = get_instrument_path('quantaq_bedroom') / 'MOD-PM-00194-b0fc215029fa4852b926bc50b28fda5a.csv'
        quantaq_k_path = get_instrument_path('quantaq_kitchen') / 'MOD-PM-00197-a6dd467a147a4d95a7b98a8a10ab4ea3.csv'

        # Process Bedroom QuantAQ
        try:
            print("Loading QuantAQB data...")
            quantaq_b_data = pd.read_csv(quantaq_b_path)

            # Convert timestamp to datetime
            quantaq_b_data["timestamp_local"] = pd.to_datetime(
                quantaq_b_data["timestamp_local"]
                .str.replace("T", " ")
                .str.replace("Z", ""),
                errors="coerce",
            ).dt.tz_localize(None)

            # Sort data by timestamp (ascending)
            quantaq_b_data = quantaq_b_data.sort_values(by="timestamp_local")

            # Process each burn
            for burn_number in quantaq_burns:
                # ... [filter by date, apply time shift, calculate time since garage, sum bins]
                # ... [~70 lines of processing per instrument]

        except (FileNotFoundError, pd.errors.ParserError, KeyError) as e:
            print(f"Error processing QuantAQB data: {str(e)}")
            traceback.print_exc()

        # [... similar code repeated for Kitchen ...]
```

### AFTER
```python
quantaq_data = {"Bedroom": {}, "Kitchen": {}}

if "QuantAQ" in selected_instruments:
    print("\nProcessing QuantAQ data...")

    # Get valid burns for QuantAQ (burns 4-10)
    from scripts.instrument_config import get_burn_range_for_instrument
    quantaq_burns = [b for b in burn_numbers if b in get_burn_range_for_instrument('QuantAQB')]

    if quantaq_burns:
        quantaq_b_path = get_instrument_path('quantaq_bedroom') / 'MOD-PM-00194-b0fc215029fa4852b926bc50b28fda5a.csv'
        quantaq_k_path = get_instrument_path('quantaq_kitchen') / 'MOD-PM-00197-a6dd467a147a4d95a7b98a8a10ab4ea3.csv'

        # Process Bedroom QuantAQ
        print("Loading QuantAQB data...")
        for burn_number in quantaq_burns:
            burn_date_row = burn_log[burn_log["Burn ID"] == burn_number]
            if not burn_date_row.empty:
                burn_date = pd.to_datetime(burn_date_row["Date"].iloc[0])
                garage_time = garage_closed_times.get(burn_number)

                filtered_data = process_quantaq_data(
                    quantaq_b_path,
                    instrument="QuantAQB",
                    burn_date=burn_date,
                    garage_closed_time=garage_time,
                    sum_bins=True,
                )

                if not filtered_data.empty:
                    quantaq_data["Bedroom"][burn_number] = filtered_data
                    print(f"  Processed QuantAQB: {burn_number} ({len(filtered_data)} records)")

        # Process Kitchen QuantAQ
        print("Loading QuantAQK data...")
        for burn_number in quantaq_burns:
            burn_date_row = burn_log[burn_log["Burn ID"] == burn_number]
            if not burn_date_row.empty:
                burn_date = pd.to_datetime(burn_date_row["Date"].iloc[0])
                garage_time = garage_closed_times.get(burn_number)

                filtered_data = process_quantaq_data(
                    quantaq_k_path,
                    instrument="QuantAQK",
                    burn_date=burn_date,
                    garage_closed_time=garage_time,
                    sum_bins=True,
                )

                if not filtered_data.empty:
                    quantaq_data["Kitchen"][burn_number] = filtered_data
                    print(f"  Processed QuantAQK: {burn_number} ({len(filtered_data)} records)")
```

**Changes:**
- ✅ ~190 lines → ~40 lines
- ✅ Use `get_burn_range_for_instrument()` for clearer intent
- ✅ All bin summing logic in `process_quantaq_data()`

---

## Step 6: Simplify SMPS Processing

### BEFORE (lines 901-1078 - ~177 lines)
```python
smps_data = {}

if "SMPS" in selected_instruments:
    print("\nProcessing SMPS data...")

    for burn_number in burn_numbers:
        # ... [load file, check if transposed, fix datetime]
        # ... [apply time shift, calculate time since garage]
        # ... [sum size ranges, ~80+ lines per burn]
```

### AFTER
```python
smps_data = {}

if "SMPS" in selected_instruments:
    print("\nProcessing SMPS data...")

    for burn_number in burn_numbers:
        burn_date_row = burn_log[burn_log["Burn ID"] == burn_number]
        if not burn_date_row.empty:
            burn_date = pd.to_datetime(burn_date_row["Date"].iloc[0])
            date_str = burn_date.strftime("%m%d%Y")
            smps_filename = f"MH_apollo_bed_{date_str}_numConc.xlsx"
            smps_path = get_instrument_path('smps') / smps_filename

            if smps_path.exists():
                garage_time = garage_closed_times.get(burn_number)

                filtered_data = process_smps_data(
                    smps_path,
                    garage_closed_time=garage_time,
                    sum_ranges=True,
                    debug=DEBUG,
                )

                if not filtered_data.empty:
                    smps_data[burn_number] = filtered_data
                    print(f"  Processed SMPS: {burn_number} ({len(filtered_data)} records)")
            else:
                print(f"  No SMPS file for {burn_number}")
```

**Changes:**
- ✅ ~177 lines → ~20 lines
- ✅ All datetime fixing, transposing, binning in `process_smps_data()`

---

## Step 7: Simplify Figure Creation

### BEFORE (create_plots_for_burn function, lines 1082-1555)
```python
def create_plots_for_burn(burn_number):
    """Create bedroom and kitchen plots for a specific burn"""
    # ... setup code

    # Define color palettes for each instrument type
    color_palettes = {"AeroTrak": Blues[9], "QuantAQ": Greens[9], "SMPS": Reds[9]}

    # Create Bedroom Plot
    bedroom_title = f"{burn_number.upper()} Bedroom Particle Count Comparison"

    tooltips = [("X", "$x{0.2f} h"), ("Y", "$y{0.2f} (#/cm³)")]

    p = figure(
        title=bedroom_title,
        x_axis_label="Time Since Garage Closed (hours)",
        y_axis_label="Particulate Matter Particle Count (#/cm³)",
        x_axis_type="linear",
        y_axis_type="log",
        width=800,
        height=600,
        tooltips=tooltips,
        tools="pan,hover,box_zoom,wheel_zoom,reset,save",
    )

    # Set y-axis limits
    p.y_range = Range1d(1e-4, 1e5)
    p.x_range = Range1d(-1, 4)

    # Improve grid appearance
    p.xgrid.grid_line_color = "lightgray"
    p.xgrid.grid_line_alpha = 0.6
    p.ygrid.grid_line_color = "lightgray"
    p.ygrid.grid_line_alpha = 0.6

    # ... [300+ more lines of plotting code]
```

### AFTER
```python
def create_plots_for_burn(burn_number):
    """Create bedroom and kitchen plots for a specific burn"""
    # ... setup code

    from bokeh.palettes import Blues, Greens, Reds
    color_palettes = {"AeroTrak": Blues[9], "QuantAQ": Greens[9], "SMPS": Reds[9]}

    # Create Bedroom Plot
    bedroom_title = f"{burn_number.upper()} Bedroom Particle Count Comparison"
    p_bedroom = create_standard_figure(
        bedroom_title,
        x_range=(-1, 4),
        y_range=(1e-4, 1e5),
    )

    # ... [plotting logic - unchanged]

    # Add event markers
    events = {}
    if burn_number in garage_closed_times:
        events['Garage Closed'] = 0
    if burn_number in cr_box_hours:
        events['CR Boxes On'] = cr_box_hours[burn_number]
    add_event_markers(p_bedroom, events, y_range=(1e-4, 1e5))

    # Configure legend
    configure_legend(p_bedroom, location='top_right', click_policy='hide')

    # ... [rest of plotting]
```

**Changes:**
- ✅ Figure creation: ~20 lines → 5 lines
- ✅ Event markers: ~30 lines → 3 lines
- ✅ Legend config: ~3 lines → 1 line

---

## Summary of Changes

### Line Count Reduction

| Section | Before | After | Savings |
|---------|--------|-------|---------|
| Utility function definitions | 343 | 0 | 343 |
| Burn log loading | 33 | 5 | 28 |
| AeroTrak processing | 213 | 35 | 178 |
| QuantAQ processing | 190 | 40 | 150 |
| SMPS processing | 177 | 20 | 157 |
| Figure creation | 60 | 10 | 50 |
| **TOTAL** | **1016** | **110** | **906 lines** |

**~89% reduction in boilerplate code for this script!**

---

## Benefits

### 1. **Readability**
- Code now focuses on analysis logic
- Intent is clearer with descriptive function names
- Less cognitive load for readers

### 2. **Maintainability**
- Bug fixes in utility functions automatically benefit all scripts
- Single source of truth for processing logic
- Easier to add new instruments or modify existing logic

### 3. **Consistency**
- All scripts use identical time shifts
- Same binning logic across analyses
- Standardized plot formatting

### 4. **Development Speed**
- New scripts can be written much faster
- Less copy-paste programming
- Focus on unique analysis requirements

---

## Migration Checklist

When migrating a script, follow these steps:

- [ ] **1. Add utility imports** at top of script
- [ ] **2. Delete utility function definitions** (search for duplicates)
- [ ] **3. Replace burn log loading** with `load_burn_log()` and helper functions
- [ ] **4. Replace instrument processing** with `process_*_data()` functions
- [ ] **5. Replace figure creation** with `create_standard_figure()`
- [ ] **6. Replace event markers** with `add_event_markers()`
- [ ] **7. Test output** - verify results match original
- [ ] **8. Clean up unused imports** (datetime, inspect, traceback, etc.)
- [ ] **9. Update documentation** if needed

---

## Testing Your Migration

After migrating, verify the script still works:

```bash
# Run the migrated script
python src/your_migrated_script.py

# Compare outputs with original
# - Check that plots look identical
# - Verify data values are the same
# - Confirm no new errors appear
```

---

## Common Pitfalls

### 1. **Import Errors**
**Problem:** `ModuleNotFoundError: No module named 'scripts'`

**Solution:** Ensure repository root is in Python path:
```python
import sys
from pathlib import Path
repo_root = Path(__file__).parent.parent
sys.path.insert(0, str(repo_root))
```

### 2. **Different Column Names**
**Problem:** Some scripts may use different datetime column names

**Solution:** Check `get_instrument_datetime_column()` in `instrument_config.py`

### 3. **Custom Processing**
**Problem:** Script has unique processing requirements

**Solution:**
- Use optional parameters in processing functions
- Or add custom processing after calling utility function

---

## Next Steps

1. **Start small:** Migrate one simple script first
2. **Test thoroughly:** Verify outputs match original
3. **Gradually adopt:** Migrate scripts as you modify them
4. **Share learnings:** Document any issues or improvements

---

**Questions or issues?** Check [scripts/README.md](README.md) for detailed documentation or review [scripts/REFACTORING_SUMMARY.md](REFACTORING_SUMMARY.md) for the full analysis.

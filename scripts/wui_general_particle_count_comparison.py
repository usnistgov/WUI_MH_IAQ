#%% 
# WUI General Particle Count Comparison
# This script creates plots comparing particle counts from multiple instruments
# for wildland-urban interface fire experiments across all burns.
# It creates separate plots for bedroom and kitchen instruments.

import os
import pandas as pd
import numpy as np
import datetime
from bokeh.plotting import figure, show
from bokeh.io import output_notebook, output_file
from bokeh.models import ColumnDataSource, Div, Range1d
from bokeh.layouts import column
from bokeh.palettes import Blues, Greens, Reds

#%% Configuration parameters

# Set paths
absolute_path = 'C:/Users/nml/OneDrive - NIST/Documents/NIST/WUI_smoke/'
output_dir = os.path.join(absolute_path, './Paper_figures/')

# Burns to process (all burns by default)
burn_numbers = [f'burn{i}' for i in range(1, 11)]  # burn1 through burn10

# Select which instruments to include in the plot
# Options: 'AeroTrak', 'QuantAQ', 'SMPS'
selected_instruments = ['AeroTrak', 'QuantAQ', 'SMPS']

# Enable debug mode for additional diagnostic output
DEBUG = False

#%% Utility functions

def get_script_metadata():
    """Return a string with script name and execution timestamp"""
    try:
        import inspect
        script_name = os.path.basename(inspect.getmodule(inspect.currentframe()).__file__)
    except (NameError, AttributeError, TypeError):
        try:
            script_name = os.path.basename(__file__)
        except NameError:
            script_name = "wui_general_particle_count_comparison.py"
           
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return f"Generated by: {script_name} | Date: {timestamp}"

def apply_time_shift(df, instrument, datetime_column):
    """
    Apply time shift to datetime column based on instrument configuration.
    
    Parameters:
    -----------
    df : DataFrame
        DataFrame containing data with datetime column
    instrument : str
        Instrument name to determine time shift
    datetime_column : str
        Name of the datetime column to shift
        
    Returns:
    --------
    DataFrame
        DataFrame with shifted datetime column
    """
    # Define time shifts for each instrument (minutes)
    time_shifts = {
        'AeroTrakB': 2.16,
        'AeroTrakK': 5,
        'QuantAQB': -2.97,
        'QuantAQK': 0,
        'SMPS': 0
    }
    
    # Get time shift for instrument
    time_shift = time_shifts.get(instrument, 0)
    
    if time_shift != 0:
        # Create a copy to avoid SettingWithCopyWarning
        df = df.copy()
        # Apply time shift
        df[datetime_column] = df[datetime_column] + pd.Timedelta(minutes=time_shift)
    
    return df

def fix_smps_datetime(smps_raw_data):
    """
    Fix the datetime creation for SMPS data.
    
    Parameters:
    -----------
    smps_raw_data : DataFrame
        DataFrame containing SMPS data with Date and Start Time columns
        
    Returns:
    --------
    DataFrame
        DataFrame with fixed datetime column
    """
    # First check if the columns exist
    if 'Date' not in smps_raw_data.columns or 'Start Time' not in smps_raw_data.columns:
        print("  Error: Date or Start Time columns missing from SMPS data")
        return smps_raw_data
    
    # Make a copy to avoid modifying the original
    data = smps_raw_data.copy()
    
    # Print sample values for debugging
    if DEBUG:
        print(f"  Sample Date value: {data['Date'].iloc[0]}")
        print(f"  Sample Start Time value: {data['Start Time'].iloc[0]}")
    
    try:
        # First try direct approach with string operations
        datetime_strings = []
        for i in range(len(data)):
            try:
                date_str = str(data['Date'].iloc[i])
                time_str = str(data['Start Time'].iloc[i])
                if date_str != 'nan' and time_str != 'nan' and date_str != 'NaT' and time_str != 'NaT':
                    datetime_strings.append(f"{date_str} {time_str}")
                else:
                    datetime_strings.append(np.nan)
            except Exception as e:
                if DEBUG:
                    print(f"  Error with row {i}: {str(e)}")
                datetime_strings.append(np.nan)
        
        data['datetime'] = pd.to_datetime(datetime_strings, errors='coerce')
        
        # If all values are NaT, try alternative approach
        if data['datetime'].isna().all():
            print("  First datetime conversion approach failed, trying alternative...")
            
            # Try to convert Date to datetime first
            data['Date'] = pd.to_datetime(data['Date'], errors='coerce')
            
            # If Start Time is already a time object, convert to string
            if pd.api.types.is_datetime64_dtype(data['Start Time']):
                data['Start Time'] = data['Start Time'].dt.strftime('%H:%M:%S')
            
            # For each row with a valid Date, combine with Start Time
            data['datetime'] = pd.NaT
            valid_dates = ~data['Date'].isna()
            
            if valid_dates.any():
                date_strings = data.loc[valid_dates, 'Date'].dt.strftime('%Y-%m-%d')
                data.loc[valid_dates, 'datetime'] = pd.to_datetime(
                    date_strings + ' ' + data.loc[valid_dates, 'Start Time'],
                    errors='coerce'
                )
    except Exception as e:
        print(f"  Error in datetime conversion: {str(e)}")
        import traceback
        traceback.print_exc()
    
    # Print stats on the new datetime column
    valid_dt_count = (~data['datetime'].isna()).sum()
    total_count = len(data)
    print(f"  Created {valid_dt_count}/{total_count} valid datetime values")
    
    return data

def calculate_time_since_garage_closed(df, datetime_column, garage_closed_time):
    """
    Calculate the time since garage closed for each row in the dataframe.
    
    Parameters:
    -----------
    df : DataFrame
        DataFrame containing data with datetime column
    datetime_column : str
        Name of the datetime column to use for calculation
    garage_closed_time : datetime
        Datetime when garage was closed
        
    Returns:
    --------
    DataFrame
        DataFrame with added 'Time Since Garage Closed (hours)' column
    """
    # If garage_closed_time is None, return the original dataframe
    if garage_closed_time is None:
        return df
        
    # Create a copy to avoid SettingWithCopyWarning
    df = df.copy()
    
    # Ensure datetime column is properly formatted
    df[datetime_column] = pd.to_datetime(df[datetime_column])
    
    # Make datetime timezone-naive if it has timezone info
    if hasattr(df[datetime_column].dtype, 'tz') and df[datetime_column].dtype.tz is not None:
        df[datetime_column] = df[datetime_column].dt.tz_localize(None)
    
    # Calculate time since garage closed in hours
    df['Time Since Garage Closed (hours)'] = (df[datetime_column] - garage_closed_time).dt.total_seconds() / 3600
    
    # Verify calculation with debug info
    if DEBUG:
        time_range = df['Time Since Garage Closed (hours)'].describe()
        print(f"  Time Since Garage Closed range: {time_range['min']:.2f} to {time_range['max']:.2f} hours")
        
        # Check if we have values in the visible range (-1 to 4 hours)
        visible_count = ((df['Time Since Garage Closed (hours)'] >= -1) & 
                          (df['Time Since Garage Closed (hours)'] <= 4)).sum()
        total_count = len(df)
        print(f"  Points in visible range (-1 to 4 hours): {visible_count}/{total_count} ({visible_count/total_count*100:.1f}%)")
    
    return df

def calculate_rolling_average_burn3(data, datetime_column):
    """
    Calculate 5-minute rolling average for burn3 data.
    
    Parameters:
    -----------
    data : DataFrame
        DataFrame containing burn3 data
    datetime_column : str
        Name of the datetime column to use as index for rolling average
        
    Returns:
    --------
    DataFrame
        DataFrame with rolling averages for numeric columns
    """
    if data.empty:
        print("No data available for burn 3.")
        return data
    
    # Set datetime column as the index for rolling average calculation
    data_indexed = data.set_index(datetime_column)
    
    # Initialize a dictionary to hold the results
    rolling_avg_data = {}
    
    # Columns to calculate rolling averages (numeric only)
    numeric_columns = data_indexed.select_dtypes(include=[np.number]).columns
    
    # Calculate rolling averages for numeric columns
    for column in numeric_columns:
        rolling_avg_data[column] = data_indexed[column].rolling(pd.Timedelta(minutes=5)).mean().astype(data_indexed[column].dtype)
    
    # For status columns, keep the first value
    status_columns = ['Flow Status', 'Instrument Status', 'Laser Status']
    for column in status_columns:
        if column in data_indexed.columns:
            rolling_avg_data[column] = data_indexed[column].iloc[0]
    
    # Create a new DataFrame with rolling averages and status values
    rolling_avg_df = pd.DataFrame(rolling_avg_data, index=data_indexed.index)
    
    # Reset index to bring datetime column back as a column
    rolling_avg_df.reset_index(inplace=True)
    
    return rolling_avg_df

def split_data_by_nan(df, x_col, y_col):
    """
    Split data into segments where NaN values occur to prevent lines connecting across gaps.
    
    Parameters:
    -----------
    df : DataFrame
        DataFrame containing the data
    x_col : str
        Column name for x values
    y_col : str
        Column name for y values
        
    Returns:
    --------
    list
        List of (x_segment, y_segment) tuples
    """
    # Drop rows with NaN in either column
    valid_data = df.dropna(subset=[x_col, y_col])
    
    if valid_data.empty:
        return []
    
    # Sort by x values
    valid_data = valid_data.sort_values(by=x_col)
    
    # Get values as numpy arrays
    x = valid_data[x_col].values
    y = valid_data[y_col].values
    
    # Check for large gaps in x values
    # (This helps prevent lines connecting distant points)
    dx = np.diff(x)
    gap_threshold = 0.1  # 6 minutes gap threshold (in hours)
    gap_indices = np.where(dx > gap_threshold)[0]
    
    # Split at gap indices
    segments = []
    start_idx = 0
    
    for gap_idx in gap_indices:
        segments.append((x[start_idx:gap_idx+1], y[start_idx:gap_idx+1]))
        start_idx = gap_idx + 1
    
    # Add the final segment
    if start_idx < len(x):
        segments.append((x[start_idx:], y[start_idx:]))
    
    return segments

def create_figure(title):
    """
    Create a standardized figure for particle count plots.
    
    Parameters:
    -----------
    title : str
        Title for the figure
        
    Returns:
    --------
    bokeh.plotting.figure
        Configured figure object
    """
    TOOLTIPS = [
        ("X","$x{0.2f} h"),
        ("Y", "$y{0.2f} (#/cm³)")
    ]

    p = figure(
        title=title,
        x_axis_label='Time Since Garage Closed (hours)',
        y_axis_label='Particulate Matter Particle Count (#/cm³)',
        x_axis_type='linear',  # Using time since garage closed
        y_axis_type='log',
        width=800,  # Same as in Update5 script
        height=600,  # Same as in Update5 script
        tooltips=TOOLTIPS,
        tools="pan,hover,box_zoom,wheel_zoom,reset,save"
    )
    
    # Set y-axis limits from comparison script
    p.y_range = Range1d(1e-4, 1e5)
    
    # Set x-axis range to match Update5 script (-1 to 4 hours)
    p.x_range.start = -1
    p.x_range.end = 4
    
    # Improve grid appearance
    p.xgrid.grid_line_color = "lightgray"
    p.xgrid.grid_line_alpha = 0.6
    p.ygrid.grid_line_color = "lightgray"
    p.ygrid.grid_line_alpha = 0.6
    
    return p

#%% Load burn log
print("Loading burn log...")
burn_log_path = os.path.join(absolute_path, 'burn_log.xlsx')
burn_log = pd.read_excel(burn_log_path, sheet_name='Sheet2')

# Create dictionary to store garage closed times for each burn
garage_closed_times = {}
cr_box_hours = {}

for burn_number in burn_numbers:
    burn_date_row = burn_log[burn_log['Burn ID'] == burn_number]
    if not burn_date_row.empty:
        burn_date = pd.to_datetime(burn_date_row['Date'].iloc[0])
        
        # Get garage closed time
        garage_closed_time_str = burn_date_row['garage closed'].iloc[0]
        if pd.notna(garage_closed_time_str):
            garage_closed_time = pd.to_datetime(f"{burn_date.strftime('%Y-%m-%d')} {garage_closed_time_str}")
            garage_closed_times[burn_number] = garage_closed_time
        
        # Get CR Box activation time
        cr_box_on_time_str = burn_date_row['CR Box on'].iloc[0]
        if pd.notna(cr_box_on_time_str):
            cr_box_on_time = pd.to_datetime(f"{burn_date.strftime('%Y-%m-%d')} {cr_box_on_time_str}")
            # Calculate hours since garage closed for CR box activation
            if burn_number in garage_closed_times:
                cr_box_hours[burn_number] = (cr_box_on_time - garage_closed_times[burn_number]).total_seconds() / 3600

#%% Process AeroTrak data
aerotrak_data = {'Bedroom': {}, 'Kitchen': {}}

if 'AeroTrak' in selected_instruments:
    print("\nProcessing AeroTrak data...")
    
    # Load bedroom AeroTrak data
    aerotrak_b_path = os.path.join(absolute_path, './burn_data/aerotraks/bedroom2/all_data.xlsx')
    aerotrak_k_path = os.path.join(absolute_path, './burn_data/aerotraks/kitchen/all_data.xlsx')
    
    # Process Bedroom AeroTrak
    try:
        print("Loading AeroTrakB data...")
        aerotrak_b_data = pd.read_excel(aerotrak_b_path)
        aerotrak_b_data.columns = aerotrak_b_data.columns.str.strip()
        
        # Define size channels and initialize a dictionary for size values
        size_channels = ['Ch1', 'Ch2', 'Ch3', 'Ch4', 'Ch5', 'Ch6']
        size_values = {}
        
        # Extract size values for each channel (from first row)
        for channel in size_channels:
            size_col = f'{channel} Size (µm)'
            if size_col in aerotrak_b_data.columns:
                size_value = aerotrak_b_data[size_col].iloc[0]
                if pd.notna(size_value):
                    size_values[channel] = size_value
        
        # Process each burn
        for burn_number in burn_numbers:
            burn_date_row = burn_log[burn_log['Burn ID'] == burn_number]
            if burn_date_row.empty:
                continue
            
            burn_date = pd.to_datetime(burn_date_row['Date'].iloc[0])
            
            # Filter data for this burn
            aerotrak_b_data['Date'] = pd.to_datetime(aerotrak_b_data['Date and Time']).dt.date
            filtered_data = aerotrak_b_data[aerotrak_b_data['Date'] == burn_date.date()].copy()
            
            if filtered_data.empty:
                print(f"No AeroTrakB data for {burn_number}")
                continue
            
            # Apply time shift
            filtered_data = apply_time_shift(filtered_data, 'AeroTrakB', 'Date and Time')
            
            # Calculate time since garage closed
            if burn_number in garage_closed_times:
                filtered_data = calculate_time_since_garage_closed(
                    filtered_data, 'Date and Time', garage_closed_times[burn_number]
                )
                
                # Apply rolling average for burn3
                if burn_number == 'burn3':
                    filtered_data = calculate_rolling_average_burn3(filtered_data, 'Date and Time')
                
                # Check instrument status columns and remove rows with invalid status
                status_columns = ['Flow Status', 'Laser Status']
                valid_status = (filtered_data[status_columns] == 'OK').all(axis=1)
                filtered_data.loc[~valid_status, filtered_data.select_dtypes(include=[np.number]).columns] = np.nan
                
                # Convert Diff counts to #/cm³
                volume_column = 'Volume (L)'
                if volume_column in filtered_data.columns:
                    filtered_data['Volume (cm³)'] = filtered_data[volume_column] * 1000  # Convert to cm³
                    
                    # Process each channel
                    for i, channel in enumerate(size_channels):
                        if channel in size_values:
                            # Get next channel's size value
                            next_channel = size_channels[i + 1] if i < len(size_channels) - 1 else None
                            next_size_value = size_values.get(next_channel, 25)  # Default to 25 if no next channel
                            
                            diff_col = f'{channel} Diff (#)'
                            if diff_col in filtered_data.columns:
                                # Create new column for particle count density
                                new_col_name = f'Ʃ{size_values[channel]}-{next_size_value}µm (#/cm³)'
                                filtered_data[new_col_name] = filtered_data[diff_col] / filtered_data['Volume (cm³)']
                
                # Store processed data
                aerotrak_data['Bedroom'][burn_number] = filtered_data
                print(f"  Processed AeroTrakB data for {burn_number}: {len(filtered_data)} records")
        
    except Exception as e:
        print(f"Error processing AeroTrakB data: {str(e)}")
        import traceback
        traceback.print_exc()
    
    # Process Kitchen AeroTrak
    try:
        print("Loading AeroTrakK data...")
        aerotrak_k_data = pd.read_excel(aerotrak_k_path)
        aerotrak_k_data.columns = aerotrak_k_data.columns.str.strip()
        
        # Define size channels and initialize a dictionary for size values
        size_channels = ['Ch1', 'Ch2', 'Ch3', 'Ch4', 'Ch5', 'Ch6']
        size_values = {}
        
        # Extract size values for each channel (from first row)
        for channel in size_channels:
            size_col = f'{channel} Size (µm)'
            if size_col in aerotrak_k_data.columns:
                size_value = aerotrak_k_data[size_col].iloc[0]
                if pd.notna(size_value):
                    size_values[channel] = size_value
        
        # Process each burn
        for burn_number in burn_numbers:
            burn_date_row = burn_log[burn_log['Burn ID'] == burn_number]
            if burn_date_row.empty:
                continue
            
            burn_date = pd.to_datetime(burn_date_row['Date'].iloc[0])
            
            # Filter data for this burn
            aerotrak_k_data['Date'] = pd.to_datetime(aerotrak_k_data['Date and Time']).dt.date
            filtered_data = aerotrak_k_data[aerotrak_k_data['Date'] == burn_date.date()].copy()
            
            if filtered_data.empty:
                print(f"No AeroTrakK data for {burn_number}")
                continue
            
            # Apply time shift
            filtered_data = apply_time_shift(filtered_data, 'AeroTrakK', 'Date and Time')
            
            # Calculate time since garage closed
            if burn_number in garage_closed_times:
                filtered_data = calculate_time_since_garage_closed(
                    filtered_data, 'Date and Time', garage_closed_times[burn_number]
                )
                
                # Apply rolling average for burn3
                if burn_number == 'burn3':
                    filtered_data = calculate_rolling_average_burn3(filtered_data, 'Date and Time')
                
                # Check instrument status columns and remove rows with invalid status
                status_columns = ['Flow Status', 'Laser Status']
                valid_status = (filtered_data[status_columns] == 'OK').all(axis=1)
                filtered_data.loc[~valid_status, filtered_data.select_dtypes(include=[np.number]).columns] = np.nan
                
                # Convert Diff counts to #/cm³
                volume_column = 'Volume (L)'
                if volume_column in filtered_data.columns:
                    filtered_data['Volume (cm³)'] = filtered_data[volume_column] * 1000  # Convert to cm³
                    
                    # Process each channel
                    for i, channel in enumerate(size_channels):
                        if channel in size_values:
                            # Get next channel's size value
                            next_channel = size_channels[i + 1] if i < len(size_channels) - 1 else None
                            next_size_value = size_values.get(next_channel, 25)  # Default to 25 if no next channel
                            
                            diff_col = f'{channel} Diff (#)'
                            if diff_col in filtered_data.columns:
                                # Create new column for particle count density
                                new_col_name = f'Ʃ{size_values[channel]}-{next_size_value}µm (#/cm³)'
                                filtered_data[new_col_name] = filtered_data[diff_col] / filtered_data['Volume (cm³)']
                
                # Store processed data
                aerotrak_data['Kitchen'][burn_number] = filtered_data
                print(f"  Processed AeroTrakK data for {burn_number}: {len(filtered_data)} records")
        
    except Exception as e:
        print(f"Error processing AeroTrakK data: {str(e)}")
        import traceback
        traceback.print_exc()

#%% Process QuantAQ data
quantaq_data = {'Bedroom': {}, 'Kitchen': {}}

if 'QuantAQ' in selected_instruments:
    print("\nProcessing QuantAQ data...")
    
    # Determine which burns have QuantAQ data
    quantaq_burns = [burn for burn in burn_numbers if burn not in ['burn1', 'burn2', 'burn3']]
    
    if quantaq_burns:
        # Load QuantAQ data for bedroom and kitchen
        quantaq_b_path = os.path.join(absolute_path, './burn_data/quantaq/MOD-PM-00194-b0fc215029fa4852b926bc50b28fda5a.csv')
        quantaq_k_path = os.path.join(absolute_path, './burn_data/quantaq/MOD-PM-00197-a6dd467a147a4d95a7b98a8a10ab4ea3.csv')
        
        # Process Bedroom QuantAQ
        try:
            print("Loading QuantAQB data...")
            quantaq_b_data = pd.read_csv(quantaq_b_path)
            
            # Convert timestamp to datetime
            quantaq_b_data['timestamp_local'] = pd.to_datetime(
                quantaq_b_data['timestamp_local'].str.replace('T', ' ').str.replace('Z', ''),
                errors='coerce'
            ).dt.tz_localize(None)
            
            # Sort data by timestamp (ascending)
            quantaq_b_data = quantaq_b_data.sort_values(by='timestamp_local')
            
            # Process each burn
            for burn_number in quantaq_burns:
                burn_date_row = burn_log[burn_log['Burn ID'] == burn_number]
                if burn_date_row.empty:
                    continue
                
                burn_date = pd.to_datetime(burn_date_row['Date'].iloc[0])
                
                # Filter data for this burn
                quantaq_b_data['Date'] = quantaq_b_data['timestamp_local'].dt.date
                filtered_data = quantaq_b_data[quantaq_b_data['Date'] == burn_date.date()].copy()
                
                if filtered_data.empty:
                    print(f"No QuantAQB data for {burn_number}")
                    continue
                
                # Apply time shift
                filtered_data = apply_time_shift(filtered_data, 'QuantAQB', 'timestamp_local')
                
                # Calculate time since garage closed
                if burn_number in garage_closed_times:
                    filtered_data = calculate_time_since_garage_closed(
                        filtered_data, 'timestamp_local', garage_closed_times[burn_number]
                    )
                    
                    # Define the bins to be summed
                    bins = {
                        'Ʃ0.35-0.66µm (#/cm³)': ['bin0', 'bin1'],
                        'Ʃ0.66-1.0µm (#/cm³)': ['bin2'],
                        'Ʃ1.0-3.0µm (#/cm³)': ['bin3', 'bin4', 'bin5', 'bin6'],
                        'Ʃ3.0-5.2µm (#/cm³)': ['bin7', 'bin8'],
                        'Ʃ5.2-10µm (#/cm³)': ['bin9', 'bin10', 'bin11'],
                        'Ʃ10-20µm (#/cm³)': ['bin12', 'bin13', 'bin14', 'bin15', 'bin16'],
                        'Ʃ20-40µm (#/cm³)': ['bin17', 'bin18', 'bin19', 'bin20', 'bin21', 'bin22', 'bin23']
                    }
                    
                    # Sum the specified bins and create new columns
                    for new_col, bin_list in bins.items():
                        if all(bin in filtered_data.columns for bin in bin_list):
                            filtered_data[new_col] = filtered_data[bin_list].sum(axis=1)
                    
                    # Store processed data
                    quantaq_data['Bedroom'][burn_number] = filtered_data
                    print(f"  Processed QuantAQB data for {burn_number}: {len(filtered_data)} records")
            
        except Exception as e:
            print(f"Error processing QuantAQB data: {str(e)}")
            import traceback
            traceback.print_exc()
        
        # Process Kitchen QuantAQ
        try:
            print("Loading QuantAQK data...")
            quantaq_k_data = pd.read_csv(quantaq_k_path)
            
            # Convert timestamp to datetime
            quantaq_k_data['timestamp_local'] = pd.to_datetime(
                quantaq_k_data['timestamp_local'].str.replace('T', ' ').str.replace('Z', ''),
                errors='coerce'
            ).dt.tz_localize(None)
            
            # Sort data by timestamp (ascending)
            quantaq_k_data = quantaq_k_data.sort_values(by='timestamp_local')
            
            # Process each burn
            for burn_number in quantaq_burns:
                burn_date_row = burn_log[burn_log['Burn ID'] == burn_number]
                if burn_date_row.empty:
                    continue
                
                burn_date = pd.to_datetime(burn_date_row['Date'].iloc[0])
                
                # Filter data for this burn
                quantaq_k_data['Date'] = quantaq_k_data['timestamp_local'].dt.date
                filtered_data = quantaq_k_data[quantaq_k_data['Date'] == burn_date.date()].copy()
                
                if filtered_data.empty:
                    print(f"No QuantAQK data for {burn_number}")
                    continue
                
                # Apply time shift
                filtered_data = apply_time_shift(filtered_data, 'QuantAQK', 'timestamp_local')
                
                # Calculate time since garage closed
                if burn_number in garage_closed_times:
                    filtered_data = calculate_time_since_garage_closed(
                        filtered_data, 'timestamp_local', garage_closed_times[burn_number]
                    )
                    
                    # Define the bins to be summed
                    bins = {
                        'Ʃ0.35-0.66µm (#/cm³)': ['bin0', 'bin1'],
                        'Ʃ0.66-1.0µm (#/cm³)': ['bin2'],
                        'Ʃ1.0-3.0µm (#/cm³)': ['bin3', 'bin4', 'bin5', 'bin6'],
                        'Ʃ3.0-5.2µm (#/cm³)': ['bin7', 'bin8'],
                        'Ʃ5.2-10µm (#/cm³)': ['bin9', 'bin10', 'bin11'],
                        'Ʃ10-20µm (#/cm³)': ['bin12', 'bin13', 'bin14', 'bin15', 'bin16'],
                        'Ʃ20-40µm (#/cm³)': ['bin17', 'bin18', 'bin19', 'bin20', 'bin21', 'bin22', 'bin23']
                    }
                    
                    # Sum the specified bins and create new columns
                    for new_col, bin_list in bins.items():
                        if all(bin in filtered_data.columns for bin in bin_list):
                            filtered_data[new_col] = filtered_data[bin_list].sum(axis=1)
                    
                    # Store processed data
                    quantaq_data['Kitchen'][burn_number] = filtered_data
                    print(f"  Processed QuantAQK data for {burn_number}: {len(filtered_data)} records")
            
        except Exception as e:
            print(f"Error processing QuantAQK data: {str(e)}")
            import traceback
            traceback.print_exc()

#%% Process SMPS data
smps_data = {}

if 'SMPS' in selected_instruments:
    print("\nProcessing SMPS data...")
    
    for burn_number in burn_numbers:
        burn_date_row = burn_log[burn_log['Burn ID'] == burn_number]
        if burn_date_row.empty:
            continue
        
        burn_date = pd.to_datetime(burn_date_row['Date'].iloc[0])
        
        # Format date for SMPS filename
        date_str = burn_date.strftime('%m%d%Y')
        smps_filename = f'MH_apollo_bed_{date_str}_numConc.xlsx'
        smps_path = os.path.join(absolute_path, f'burn_data/smps/{smps_filename}')
        
        try:
            # Check if file exists
            if not os.path.exists(smps_path):
                print(f"No SMPS data file for {burn_number}: {smps_path}")
                continue
            
            print(f"Loading SMPS data for {burn_number}...")
            
            # Load SMPS data - try both sheet names
            try:
                smps_raw_data = pd.read_excel(smps_path, sheet_name='all_data')
            except:
                try:
                    smps_raw_data = pd.read_excel(smps_path, sheet_name='sheet1')
                except:
                    # Default to first sheet
                    smps_raw_data = pd.read_excel(smps_path)
            
            # Enhanced debug info about SMPS data
            if DEBUG:
                print(f"\n  SMPS raw data for {burn_number}:")
                print(f"  Shape: {smps_raw_data.shape}")
                print(f"  First few columns: {smps_raw_data.columns[:5].tolist()}")
                print(f"  Column types: {smps_raw_data.dtypes.head()}")
            
            # Check for structured data vs transpose needed
            if 'Date' not in smps_raw_data.columns or 'Start Time' not in smps_raw_data.columns:
                # Data might be transposed, try extracting from first row
                if isinstance(smps_raw_data.iloc[0].values, np.ndarray) and 'Date' in smps_raw_data.iloc[0].values and 'Start Time' in smps_raw_data.iloc[0].values:
                    print(f"  SMPS data appears to be transposed for {burn_number}, fixing...")
                    smps_raw_data = smps_raw_data.transpose()
                    smps_raw_data.columns = smps_raw_data.iloc[0].values
                    smps_raw_data = smps_raw_data.iloc[1:].reset_index(drop=True)
            
            # Drop the 'Total Concentration(#/cm³)' column if it exists
            if 'Total Concentration(#/cm³)' in smps_raw_data.columns:
                smps_raw_data.drop(columns=['Total Concentration(#/cm³)'], inplace=True)
            
            # Print sample values before conversion
            if DEBUG:
                print(f"\n  SMPS data sample for {burn_number} before conversion:")
                print(f"  Date column type: {smps_raw_data['Date'].dtype}")
                print(f"  Start Time column type: {smps_raw_data['Start Time'].dtype}")
                print(f"  Sample Date values: {smps_raw_data['Date'].head(3).tolist()}")
                print(f"  Sample Start Time values: {smps_raw_data['Start Time'].head(3).tolist()}")
            
            # Fix SMPS datetime using the new function
            smps_raw_data = fix_smps_datetime(smps_raw_data)
            
            # Apply time shift
            smps_raw_data = apply_time_shift(smps_raw_data, 'SMPS', 'datetime')
            
            # Calculate time since garage closed
            if burn_number in garage_closed_times:
                smps_raw_data = calculate_time_since_garage_closed(
                    smps_raw_data, 'datetime', garage_closed_times[burn_number]
                )
                
                # Define bin ranges - using same ranges as update5 script
                bin_ranges = [(9, 100), (100, 200), (200, 300), (300, 437)]
                
                # Sum columns based on specified ranges
                for start, end in bin_ranges:
                    # Identify columns that can be converted to float
                    numeric_cols = []
                    for col in smps_raw_data.columns:
                        try:
                            val = float(col)
                            if start <= val <= end:
                                numeric_cols.append(col)
                        except (ValueError, TypeError):
                            continue
                    
                    if numeric_cols:
                        # Add diagnostic info about the numeric columns being summed
                        if DEBUG:
                            print(f"  Range {start}-{end}nm has {len(numeric_cols)} columns")
                            
                        # Create the summed column
                        bin_column_name = f'Ʃ{start}-{end}nm (#/cm³)'
                        smps_raw_data[bin_column_name] = smps_raw_data[numeric_cols].sum(axis=1)
                        
                        # Add diagnostic info about the summed column
                        if DEBUG:
                            stats = smps_raw_data[bin_column_name].describe()
                            print(f"  Stats for {bin_column_name}: min={stats['min']:.2f}, mean={stats['mean']:.2f}, max={stats['max']:.2f}")
                            print(f"  NaN count: {smps_raw_data[bin_column_name].isna().sum()}/{len(smps_raw_data)}")
                
                # Print debug info about SMPS data
                particle_cols = [col for col in smps_raw_data.columns if isinstance(col, str) and '(#/cm³)' in col]
                print(f"  SMPS bin columns for {burn_number}: {particle_cols}")
                
                # More detailed diagnostic info
                if DEBUG:
                    print(f"\n  SMPS Data Quality Check for {burn_number}:")
                    
                    # Check if there are any negative values in the particle columns
                    for col in particle_cols:
                        neg_count = (smps_raw_data[col] < 0).sum()
                        if neg_count > 0:
                            print(f"    Warning: {col} has {neg_count} negative values")
                    
                    # Check if Time Since Garage Closed has good distribution
                    time_col = 'Time Since Garage Closed (hours)'
                    if time_col in smps_raw_data.columns:
                        time_range = smps_raw_data[time_col].describe()
                        print(f"    {time_col} stats: min={time_range['min']:.2f}, 25%={time_range['25%']:.2f}, "
                              f"median={time_range['50%']:.2f}, 75%={time_range['75%']:.2f}, max={time_range['max']:.2f}")
                    
                    # Print sample data rows
                    try:
                        print("\n    Sample rows (first few):")
                        sample_columns = ['Time Since Garage Closed (hours)'] + particle_cols
                        sample_data = smps_raw_data[sample_columns].head()
                        for _, row in sample_data.iterrows():
                            print(f"      Time: {row[time_col]:.2f}h, Values: " + ", ".join([f"{col}={row[col]:.2f}" for col in particle_cols]))
                    except Exception as e:
                        print(f"    Error printing sample rows: {str(e)}")
                
                # Store processed data (SMPS is always in bedroom)
                smps_data[burn_number] = smps_raw_data
                print(f"  Processed SMPS data for {burn_number}: {len(smps_raw_data)} records")
            
        except Exception as e:
            print(f"Error processing SMPS data for {burn_number}: {str(e)}")
            import traceback
            traceback.print_exc()

#%% Create plots for each burn
def create_plots_for_burn(burn_number):
    """Create bedroom and kitchen plots for a specific burn"""
    print(f"\n{'-'*50}")
    print(f"Creating plots for {burn_number}")
    print(f"{'-'*50}")
    
    # Skip if no garage closed time available
    if burn_number not in garage_closed_times:
        print(f"Skipping {burn_number}: No garage closed time")
        return
        
    # Define color palettes for each instrument type
    color_palettes = {
        'AeroTrak': Blues[9],
        'QuantAQ': Greens[9],
        'SMPS': Reds[9]
    }

    # Create Bedroom Plot
    bedroom_data = {}
    if 'AeroTrak' in selected_instruments and burn_number in aerotrak_data['Bedroom']:
        bedroom_data['AeroTrak'] = aerotrak_data['Bedroom'][burn_number]
    if 'QuantAQ' in selected_instruments and burn_number in quantaq_data['Bedroom']:
        bedroom_data['QuantAQ'] = quantaq_data['Bedroom'][burn_number]
    if 'SMPS' in selected_instruments and burn_number in smps_data:
        bedroom_data['SMPS'] = smps_data[burn_number]
    
    if bedroom_data:
        # Create bedroom figure
        bedroom_title = f"{burn_number.upper()} Bedroom Particle Count Comparison"
        p_bedroom = create_figure(bedroom_title)
        
        # Process and plot each instrument's data
        for instrument_name, instrument_data in bedroom_data.items():
            # Add instrument name to legend first (as a dummy line)
            p_bedroom.line([], [], legend_label=instrument_name, line_width=0)
            
            # Enhanced debug for SMPS
            if instrument_name == 'SMPS' and DEBUG:
                print(f"\nDEBUG: Processing SMPS data for {burn_number}")
                print(f"Shape: {instrument_data.shape}")
                print(f"Columns: {instrument_data.columns.tolist()}")
                time_col = 'Time Since Garage Closed (hours)'
                if time_col in instrument_data.columns:
                    time_stats = instrument_data[time_col].describe()
                    print(f"{time_col} stats: min={time_stats['min']:.2f}, mean={time_stats['mean']:.2f}, max={time_stats['max']:.2f}")
                    # Check for NaN values
                    nan_count = instrument_data[time_col].isna().sum()
                    print(f"NaN count in {time_col}: {nan_count}/{len(instrument_data)} ({nan_count/len(instrument_data)*100:.1f}%)")
                
            # Find columns with particle count data
            particle_cols = []
            for col in instrument_data.columns:
                if isinstance(col, str) and '(#/cm³)' in col:
                    particle_cols.append(col)
            
            # Sort particle columns by size
            def extract_size(col_name):
                try:
                    # Extract the size range (e.g., "Ʃ0.5-1.0µm" -> [0.5, 1.0])
                    if 'nm' in col_name:
                        parts = col_name.replace('Ʃ', '').replace('nm (#/cm³)', '').split('-')
                    else:
                        parts = col_name.replace('Ʃ', '').replace('µm (#/cm³)', '').split('-')
                    return float(parts[0])
                except (ValueError, IndexError):
                    return float('inf')  # Put columns with parsing errors at the end
            
            # Sort columns by size
            particle_cols.sort(key=extract_size)
            
            # Debug: Check if we have particle columns for this instrument
            if not particle_cols:
                print(f"Warning: No particle columns found for {instrument_name} in {burn_number} bedroom data")
                if instrument_name == 'SMPS':
                    # Print all columns for debugging SMPS data
                    print(f"SMPS columns: {instrument_data.columns.tolist()}")
                    # Try to force add some SMPS data columns if possible
                    for col in instrument_data.columns:
                        if isinstance(col, (int, float)) or (isinstance(col, str) and col.replace('.', '', 1).isdigit()):
                            # This is likely a numeric size column from SMPS
                            numeric_col = float(col) if isinstance(col, str) else col
                            if 9 <= numeric_col <= 437:  # Within typical SMPS range
                                particle_cols.append(col)
                continue
            
            # Plot each particle size range
            for i, col in enumerate(particle_cols):
                # Get color from palette
                color = color_palettes[instrument_name][i % len(color_palettes[instrument_name])]
                
                # Special handling for SMPS data
                if instrument_name == 'SMPS':
                    try:
                        # Get time and data columns with extra checks
                        time_column = 'Time Since Garage Closed (hours)'
                        
                        # Check if both columns exist
                        if time_column not in instrument_data.columns:
                            print(f"Error: Time column missing for SMPS in {burn_number}")
                            continue
                        if col not in instrument_data.columns:
                            print(f"Error: Data column {col} missing for SMPS in {burn_number}")
                            continue
                        
                        # Get time and concentration values
                        time_values = instrument_data[time_column].values
                        concentration_values = instrument_data[col].values
                        
                        # Check if all time values are NaN
                        if np.all(np.isnan(time_values)):
                            print(f"Warning: All time values are NaN for {burn_number} SMPS data")
                            print("This might be due to an issue with datetime conversion.")
                            print("Check the 'Date' and 'Start Time' formats in the source file.")
                            continue
                        
                        # Filter out NaN, zero, and negative values
                        valid_mask = (~np.isnan(time_values)) & (~np.isnan(concentration_values)) & (concentration_values > 0)
                        
                        # Check if we have valid data
                        if np.sum(valid_mask) < 2:
                            print(f"Warning: Not enough valid data points for SMPS {col} in {burn_number}")
                            if DEBUG:
                                print(f"Total points: {len(time_values)}")
                                print(f"NaN in time: {np.sum(np.isnan(time_values))}")
                                print(f"NaN in concentration: {np.sum(np.isnan(concentration_values))}")
                                print(f"Non-positive concentration: {np.sum(concentration_values <= 0)}")
                            continue
                        
                        # Get valid data points
                        x_values = time_values[valid_mask]
                        y_values = concentration_values[valid_mask]
                        
                        # Debug check if we have any x_values in the visible range
                        if DEBUG:
                            in_range = np.sum((x_values >= -1) & (x_values <= 4))
                            total = len(x_values)
                            print(f"SMPS {col}: {in_range}/{total} points in visible range (-1 to 4 hours)")
                            
                            # Print some sample values for verification
                            if len(x_values) > 0:
                                print("Sample values (time, concentration):")
                                for i in range(min(5, len(x_values))):
                                    print(f"  {x_values[i]:.2f}, {y_values[i]:.2f}")
                        
                        # Only proceed if we have data points in range
                        in_range_mask = (x_values >= -1) & (x_values <= 4)
                        if np.sum(in_range_mask) < 2:
                            print(f"Warning: Not enough in-range data points for SMPS {col} in {burn_number}")
                            continue
                        
                        # Get in-range values
                        x_in_range = x_values[in_range_mask]
                        y_in_range = y_values[in_range_mask]
                        
                        # Sort by x values
                        sort_indices = np.argsort(x_in_range)
                        x_sorted = x_in_range[sort_indices]
                        y_sorted = y_in_range[sort_indices]
                        
                        # Format legend label
                        legend_label = col.replace(' (#/cm³)', '')
                        
                        # Plot directly
                        p_bedroom.line(x_sorted, y_sorted, 
                                 legend_label=legend_label, 
                                 line_width=2,
                                 color=color, 
                                 line_dash='solid')
                                #line_dash='dotted')
                        
                        if DEBUG:
                            print(f"Successfully plotted SMPS {col} for {burn_number}")
                            
                    except Exception as e:
                        print(f"Error plotting SMPS {col} for {burn_number}: {str(e)}")
                        import traceback
                        traceback.print_exc()
                        continue
                    
                else:
                    # Standard handling for other instruments
                    try:
                        # Create a temporary dataframe with x, y columns
                        temp_df = pd.DataFrame({
                            'x': instrument_data['Time Since Garage Closed (hours)'],
                            'y': instrument_data[col]
                        })
                        
                        # Split data by NaNs to prevent lines connecting across gaps
                        segments = split_data_by_nan(temp_df, 'x', 'y')
                        
                        # Format legend label (just the size range without units)
                        legend_label = col.replace(' (#/cm³)', '')
                        
                        # Plot each segment
                        for j, (segment_x, segment_y) in enumerate(segments):
                            if len(segment_x) > 1:  # Only plot segments with multiple points
                                # Only add to legend for first segment of each column
                                p_bedroom.line(segment_x, segment_y, 
                                        legend_label=(legend_label if j == 0 else None), 
                                        line_width=2, color=color, 
                                        line_dash='solid')
                                        #line_dash='solid' if instrument_name == 'AeroTrak' else 'dashed')
                    except Exception as e:
                        print(f"Error plotting {instrument_name} {col} for {burn_number}: {str(e)}")
                        continue
        
        # Add vertical lines and text
        p_bedroom.line(x=[0, 0], y=[1e-4, 1e5], line_color='black', line_width=2, 
                  legend_label="Garage Closed", line_dash='solid')
        p_bedroom.text(x=[0], y=[1e-1], text=["Garage Closed"], 
                  text_align="right", text_baseline="middle", 
                  text_color="black", font_size="10pt")
        
        if burn_number in cr_box_hours:
            p_bedroom.line(x=[cr_box_hours[burn_number], cr_box_hours[burn_number]], y=[1e-4, 1e5], line_color='black', line_width=2, 
                      legend_label="CR Boxes On", line_dash='dashed')
            p_bedroom.text(x=[cr_box_hours[burn_number]], y=[1e-2], text=["CR Boxes On"], 
                      text_align="left", text_baseline="middle", 
                      text_color="black", font_size="10pt")
        
        # Configure the legend
        p_bedroom.legend.location = "top_right"
        p_bedroom.legend.click_policy = "hide"
        p_bedroom.legend.background_fill_alpha = 0.7
        
        # Add metadata
        metadata = get_script_metadata()
        bedroom_instruments_list = list(bedroom_data.keys())
        instruments_str = ", ".join(bedroom_instruments_list)
        bedroom_info_div = Div(
            text=f"<p><b>{burn_number.upper()} Bedroom</b> | Instruments: {instruments_str}<br>"
                 f"<small>{metadata}</small></p>",
            width=800
        )
        
        # Create the bedroom layout
        bedroom_layout = column(p_bedroom, bedroom_info_div)
        
        # Show bedroom plot
        show(bedroom_layout)
        
        output_file_path = os.path.join(output_dir, f'{burn_number}_bedroom_particle_count_comparison.html')
        output_file(output_file_path)
        print(f"Bedroom figure would be saved to: {output_file_path}")
    
    # Create Kitchen Plot
    kitchen_data = {}
    if 'AeroTrak' in selected_instruments and burn_number in aerotrak_data['Kitchen']:
        kitchen_data['AeroTrak'] = aerotrak_data['Kitchen'][burn_number]
    if 'QuantAQ' in selected_instruments and burn_number in quantaq_data['Kitchen']:
        kitchen_data['QuantAQ'] = quantaq_data['Kitchen'][burn_number]
    
    if kitchen_data:
        # Create kitchen figure
        kitchen_title = f"{burn_number.upper()} Kitchen Particle Count Comparison"
        p_kitchen = create_figure(kitchen_title)
        
        # Plot each kitchen instrument
        for instrument_name, instrument_data in kitchen_data.items():
            # Add instrument name to legend first (as a dummy line)
            p_kitchen.line([], [], legend_label=instrument_name, line_width=0)
            
            # Find columns with particle count data
            particle_cols = []
            for col in instrument_data.columns:
                if isinstance(col, str) and '(#/cm³)' in col:
                    particle_cols.append(col)
            
            # Sort particle columns by size
            def extract_size(col_name):
                try:
                    # Extract the size range (e.g., "Ʃ0.5-1.0µm" -> [0.5, 1.0])
                    parts = col_name.replace('Ʃ', '').replace('µm (#/cm³)', '').split('-')
                    return float(parts[0])
                except (ValueError, IndexError):
                    return float('inf')  # Put columns with parsing errors at the end
                    
            particle_cols.sort(key=extract_size)
            
            # Debug: Check if we have particle columns for this instrument
            if not particle_cols:
                print(f"Warning: No particle columns found for {instrument_name} in {burn_number} kitchen data")
                print(f"Available columns: {instrument_data.columns.tolist()}")
                continue
                
            # Plot each particle size range
            for i, col in enumerate(particle_cols):
                # Get color from palette
                color = color_palettes[instrument_name][i % len(color_palettes[instrument_name])]
                
                # Create a temporary dataframe with x, y columns
                temp_df = pd.DataFrame({
                    'x': instrument_data['Time Since Garage Closed (hours)'],
                    'y': instrument_data[col]
                })
                
                # Split data by NaNs to prevent lines connecting across gaps
                segments = split_data_by_nan(temp_df, 'x', 'y')
                
                # Format legend label (just the size range)
                legend_label = col.replace(' (#/cm³)', '')
                
                # Plot each segment
                for j, (segment_x, segment_y) in enumerate(segments):
                    if len(segment_x) > 1:  # Only plot segments with multiple points
                        # Only add to legend for first segment of each column
                        p_kitchen.line(segment_x, segment_y, 
                                legend_label=(legend_label if j == 0 else None), 
                                line_width=2, color=color, 
                                line_dash='solid')
                                #line_dash='solid' if instrument_name == 'AeroTrak' else 'dashed')
        
        # Add vertical lines and text
        p_kitchen.line(x=[0, 0], y=[1e-4, 1e5], line_color='black', line_width=2, 
                  legend_label="Garage Closed", line_dash='solid')
        p_kitchen.text(x=[0], y=[1e-1], text=["Garage Closed"], 
                  text_align="right", text_baseline="middle", 
                  text_color="black", font_size="10pt")
        
        if burn_number in cr_box_hours:
            p_kitchen.line(x=[cr_box_hours[burn_number], cr_box_hours[burn_number]], y=[1e-4, 1e5], line_color='black', line_width=2, 
                      legend_label="CR Boxes On", line_dash='dashed')
            p_kitchen.text(x=[cr_box_hours[burn_number]], y=[1e-2], text=["CR Boxes On"], 
                      text_align="left", text_baseline="middle", 
                      text_color="black", font_size="10pt")
        
        # Configure the legend
        p_kitchen.legend.location = "top_right"
        p_kitchen.legend.click_policy = "hide"
        p_kitchen.legend.background_fill_alpha = 0.7
        
        # Add metadata
        metadata = get_script_metadata()
        kitchen_instruments_list = list(kitchen_data.keys())
        instruments_str = ", ".join(kitchen_instruments_list)
        kitchen_info_div = Div(
            text=f"<p><b>{burn_number.upper()} Kitchen</b> | Instruments: {instruments_str}<br>"
                 f"<small>{metadata}</small></p>",
            width=800
        )
        
        # Create the kitchen layout
        kitchen_layout = column(p_kitchen, kitchen_info_div)
        
        # Show kitchen plot
        show(kitchen_layout)
        
        output_file_path = os.path.join(output_dir, f'{burn_number}_kitchen_particle_count_comparison.html')
        output_file(output_file_path)
        print(f"Kitchen figure would be saved to: {output_file_path}")

#%% Main execution
# Prepare to output the plot in notebook
output_notebook()

# Process each burn
for burn_number in burn_numbers:
    create_plots_for_burn(burn_number)
# %%
